{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Forest Fire* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/162/forest+fires). Extract the data files into the subdirectory: `../data/fires/` (relative to `./src/`).\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ The model objective is to predict the area affected by forest fires given the features set. \n",
    "+ The objective of this exercise is to assess your ability to construct and evaluate model pipelines.\n",
    "+ Please note: the instructions are not meant to be 100% prescriptive, but instead they are a set of minimum requirements. If you find predictive performance gains by applying additional steps, by all means show them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Description\n",
    "\n",
    "From the description file contained in the archive (`forestfires.names`), we obtain the following variable descriptions:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: \"jan\" to \"dec\" \n",
    "4. day - day of the week: \"mon\" to \"sun\"\n",
    "5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specific Tasks\n",
    "\n",
    "+ Construct four model pipelines, out of combinations of the following components:\n",
    "\n",
    "    + Preprocessors:\n",
    "\n",
    "        - A simple processor that only scales numeric variables and recodes categorical variables.\n",
    "        - A transformation preprocessor that scales numeric variables and applies a non-linear transformation.\n",
    "    \n",
    "    + Regressor:\n",
    "\n",
    "        - A baseline regressor, which could be a [K-nearest neighbours model](https://open.spotify.com/track/4R3AU2pjv8ge2siX1fVbZs?si=b2712f32da0e4358) or a simple [linear regression model](https://scikit-learn.org/stable/modules/linear_model.html)\n",
    "        - An advanced regressor of your choice (e.g., Random Forest, Neural Network, etc.)\n",
    "\n",
    "+ Evaluate tune and evaluate each of the four model pipelines. \n",
    "\n",
    "    - Select a [performance metric](https://scikit-learn.org/stable/modules/linear_model.html) out of the following options: explained variance, max error, root mean squared error (RMSE), mean absolute error (MAE), r-squared.\n",
    "    - *TIPS*: \n",
    "    \n",
    "        * Out of the suggested metrics above, [some are correlation metrics, but this is a prediction problem](https://www.tmwr.org/performance#performance). Choose wisely (and don't choose the incorrect options.) \n",
    "\n",
    "+ Select the best-performing model and explain its predictions.\n",
    "\n",
    "    - Provide local explanations.\n",
    "    - Obtain global explanations and recommend a variable selection strategy.\n",
    "\n",
    "+ Export your model as a pickle file.\n",
    "\n",
    "\n",
    "You can work on the Jupyter notebook, as this experiment is fairly short (no need to use sacred). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'coord_x', 'coord_y', 'month', 'day', 'ffmc', 'dmc', 'dc', 'isi', 'temp', 'rh', 'wind', 'rain', 'area' \n",
    "]\n",
    "fires_dt = (pd.read_csv('C:/Users/Navneet Choudhary/OneDrive/Documents/DSI/Scaling to Production/scaling_to_production/05_src/data/forestfires.csv', header = None, names = columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['coord_x', 'coord_y', 'month', 'day', 'ffmc', 'dmc', 'dc', 'isi',\n",
      "       'temp', 'rh', 'wind', 'rain', 'area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = fires_dt.drop(columns=['area'])  \n",
    "y = fires_dt['area'] \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(fires_dt.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH',\n",
      "       'wind', 'rain', 'area'],\n",
      "      dtype='object')\n",
      "Pipeline 1: RMSE = 107.77\n",
      "Pipeline 2: RMSE = 109.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your dataset (replace 'fires_dataset.csv' with your actual dataset path)\n",
    "fires_dt = pd.read_csv('C:/Users/Navneet Choudhary/OneDrive/Documents/DSI/Scaling to Production/scaling_to_production/05_src/data/forestfires.csv')\n",
    "\n",
    "# Double-check columns after loading the dataset\n",
    "print(fires_dt.columns)\n",
    "\n",
    "# Verify if 'area' column is present\n",
    "if 'area' in fires_dt.columns:\n",
    "    # Drop 'area' column to create X and keep 'area' column for y\n",
    "    X = fires_dt.drop(columns=['area'])\n",
    "    y = fires_dt['area']\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define a function to calculate RMSE\n",
    "    def rmse(y_true, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # Define preprocessors for numeric and categorical columns\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessors using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, X.select_dtypes(include=np.number).columns),\n",
    "        ('cat', categorical_transformer, X.select_dtypes(include='object').columns)\n",
    "    ])\n",
    "\n",
    "    # Define pipelines with different regressors\n",
    "    pipeline1 = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    pipeline2 = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Store pipelines in a dictionary\n",
    "    pipelines = {\n",
    "        'Pipeline 1': pipeline1,\n",
    "        'Pipeline 2': pipeline2,\n",
    "    }\n",
    "\n",
    "    # Evaluate each pipeline\n",
    "    results = {}\n",
    "    for pipe_name, pipeline in pipelines.items():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        rmse_val = rmse(y_test, y_pred)\n",
    "        results[pipe_name] = rmse_val\n",
    "\n",
    "    # Print results\n",
    "    for pipe_name, rmse_val in results.items():\n",
    "        print(f\"{pipe_name}: RMSE = {rmse_val:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   X  Y month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
      "0  7  5   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
      "1  7  4   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
      "2  7  4   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
      "3  8  6   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
      "4  8  6   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0\n",
      "X          int64\n",
      "Y          int64\n",
      "month     object\n",
      "day       object\n",
      "FFMC     float64\n",
      "DMC      float64\n",
      "DC       float64\n",
      "ISI      float64\n",
      "temp     float64\n",
      "RH         int64\n",
      "wind     float64\n",
      "rain     float64\n",
      "area     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(fires_dt.head())\n",
    "print(fires_dt.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH',\n",
      "       'wind', 'rain', 'area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(fires_dt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     X  Y month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain\n",
      "0    7  5   mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0\n",
      "1    7  4   oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0\n",
      "2    7  4   oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0\n",
      "3    8  6   mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2\n",
      "4    8  6   mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0\n",
      "..  .. ..   ...  ...   ...    ...    ...   ...   ...  ..   ...   ...\n",
      "512  4  3   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0\n",
      "513  2  4   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0\n",
      "514  7  4   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0\n",
      "515  1  4   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0\n",
      "516  6  3   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0\n",
      "\n",
      "[517 rows x 12 columns]\n",
      "0       0.00\n",
      "1       0.00\n",
      "2       0.00\n",
      "3       0.00\n",
      "4       0.00\n",
      "       ...  \n",
      "512     6.44\n",
      "513    54.29\n",
      "514    11.16\n",
      "515     0.00\n",
      "516     0.00\n",
      "Name: area, Length: 517, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = fires_dt.drop(columns=['area'])\n",
    "\n",
    "y = fires_dt['area']\n",
    "\n",
    "print(X)\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create two [Column Transformers](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), called preproc1 and preproc2, with the following guidelines:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * (Preproc 1 and 2) Scaling: use a scaling method of your choice (Standard, Robust, Min-Max). \n",
    "    * Preproc 2 only: \n",
    "        \n",
    "        + Choose a transformation for any of your input variables (or several of them). Evaluate if this transformation is convenient.\n",
    "        + The choice of scaler is up to you.\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * (Preproc 1 and 2) Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) where appropriate.\n",
    "\n",
    "\n",
    "+ The only difference between preproc1 and preproc2 is the non-linear transformation of the numerical variables.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 1\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, no other transforms.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_cols = ['X', 'Y', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'area']\n",
    "categorical_cols = ['month', 'day']\n",
    "\n",
    "\n",
    "preproc1 = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 2\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, non-linear transformation to one or more variables.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc2 = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),  \n",
    "        ('transform', FunctionTransformer(np.log1p, validate=True))  \n",
    "    ]), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `regressor` and assign a regression model to it. \n",
    "\n",
    "## Regressor\n",
    "\n",
    "+ Use a regression model to perform a prediction. \n",
    "\n",
    "    - Choose a baseline regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Choose a more advance regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Both model choices are up to you, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preproc2),  \n",
    "    ('regressor', RandomForestRegressor(random_state=42))  \n",
    "])\n",
    "\n",
    "\n",
    "baseline_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preproc2),  \n",
    "    ('regressor', LinearRegression()) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preproc1 = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n",
    "\n",
    "\n",
    "preproc2 = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),  \n",
    "        ('transform', FunctionTransformer(func=np.log1p, validate=True))  \n",
    "    ]), numeric_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessing', preproc1),  \n",
    "    ('regressor', RandomForestRegressor(random_state=42))  \n",
    "])\n",
    "\n",
    "\n",
    "param_grid2 = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline2 = Pipeline(steps=[\n",
    "    ('preprocessing', preproc1),  \n",
    "    ('regressor', RandomForestRegressor(random_state=42))  \n",
    "])\n",
    "\n",
    "\n",
    "param_grid2 = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline3 = Pipeline(steps=[\n",
    "    ('preprocessing', preproc2),  \n",
    "    ('regressor', LinearRegression())  \n",
    "])\n",
    "\n",
    "\n",
    "param_grid3 = {\n",
    "    'regressor__normalize': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline4 = Pipeline(steps=[\n",
    "    ('preprocessing', preproc2),  \n",
    "    ('regressor', RandomForestRegressor(random_state=42))  \n",
    "])\n",
    "\n",
    "\n",
    "param_grid4 = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams\n",
    "\n",
    "+ Perform GridSearch on each of the four pipelines. \n",
    "+ Tune at least one hyperparameter per pipeline.\n",
    "+ Experiment with at least four value combinations per pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "baseline_regressor = LinearRegression()\n",
    "\n",
    "\n",
    "advanced_regressor = RandomForestRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', baseline_regressor)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', advanced_regressor)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline3 = Pipeline([\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', baseline_regressor)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline4 = Pipeline([\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', advanced_regressor)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid1 = {\n",
    "    'regressor__fit_intercept': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid2 = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid3 = {\n",
    "    'regressor__fit_intercept': [True, False]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid4 = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV] END ......................regressor__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ......................regressor__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ......................regressor__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ......................regressor__fit_intercept=True; total time=   0.0s\n",
      "[CV] END ......................regressor__fit_intercept=True; total time=   0.0s\n",
      "[CV] END .....................regressor__fit_intercept=False; total time=   0.0s\n",
      "[CV] END .....................regressor__fit_intercept=False; total time=   0.0s\n",
      "[CV] END .....................regressor__fit_intercept=False; total time=   0.0s\n",
      "[CV] END .....................regressor__fit_intercept=False; total time=   0.0s\n",
      "[CV] END .....................regressor__fit_intercept=False; total time=   0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'area'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 361, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'area'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 472, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 409, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 1329, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 313, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 969, in fit_transform\n    self._validate_column_callables(X)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 536, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 369, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[600], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      4\u001b[0m grid_search1 \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipeline1, param_grid\u001b[38;5;241m=\u001b[39mparam_grid1, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mgrid_search1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m grid_search2 \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipeline2, param_grid\u001b[38;5;241m=\u001b[39mparam_grid2, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m grid_search2\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_search.py:945\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    943\u001b[0m     )\n\u001b[1;32m--> 945\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 10 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas\\\\_libs\\\\hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'area'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 361, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 'area'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 472, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 409, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\pipeline.py\", line 1329, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 313, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 969, in fit_transform\n    self._validate_column_callables(X)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 536, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"c:\\Users\\Navneet Choudhary\\AppData\\Local\\miniconda3\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 369, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "grid_search1 = GridSearchCV(estimator=pipeline1, param_grid=param_grid1, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "grid_search2 = GridSearchCV(estimator=pipeline2, param_grid=param_grid2, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "grid_search3 = GridSearchCV(estimator=pipeline3, param_grid=param_grid3, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search3.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "grid_search4 = GridSearchCV(estimator=pipeline4, param_grid=param_grid4, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search4.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Pipeline 1:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[500], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Pipeline 1 - Best parameters and RMSE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for Pipeline 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgrid_search1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_params_\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest RMSE for Pipeline 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m-\u001b[39mgrid_search1\u001b[38;5;241m.\u001b[39mbest_score_))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Pipeline 2 - Best parameters and RMSE\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Best parameters for Pipeline 1:\")\n",
    "print(grid_search1.best_params_)\n",
    "print(\"Best RMSE for Pipeline 1:\", np.sqrt(-grid_search1.best_score_))\n",
    "\n",
    "\n",
    "print(\"Best parameters for Pipeline 2:\")\n",
    "print(grid_search2.best_params_)\n",
    "print(\"Best RMSE for Pipeline 2:\", np.sqrt(-grid_search2.best_score_))\n",
    "\n",
    "print(\"Best parameters for Pipeline 3:\")\n",
    "print(grid_search3.best_params_)\n",
    "print(\"Best RMSE for Pipeline 3:\", np.sqrt(-grid_search3.best_score_))\n",
    "\n",
    "print(\"Best parameters for Pipeline 4:\")\n",
    "print(grid_search4.best_params_)\n",
    "print(\"Best RMSE for Pipeline 4:\", np.sqrt(-grid_search4.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "+ Which model has the best performance?\n",
    "\n",
    "Grid Search has the best performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "+ Save the best performing model to a pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain\n",
    "\n",
    "+ Use SHAP values to explain the following only for the best-performing model:\n",
    "\n",
    "    - Select an observation in your test set and explain which are the most important features that explain that observation's specific prediction.\n",
    "\n",
    "    - In general, across the complete training set, which features are the most and least important.\n",
    "\n",
    "+ If you were to remove features from the model, which ones would you remove? Why? How would you test that these features are actually enhancing model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-3`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_3.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Cortez,Paulo and Morais,Anbal. (2008). Forest Fires. UCI Machine Learning Repository. https://doi.org/10.24432/C5D88D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
